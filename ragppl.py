# -*- coding: utf-8 -*-
"""RAGPPL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qtNi_PM_ZOXMWbrLx8-KZmfk5Limftdy
"""

!pip install pdfplumber faiss-cpu transformers torchvision pillow

from google.colab import files
uploaded = files.upload()

import pdfplumber
from PIL import Image
import os

os.makedirs("images", exist_ok=True)

pdf_path = "/content/Res2Net Forgery detection approach.pdf"
text_chunks = []
image_paths = []

with pdfplumber.open(pdf_path) as pdf:
    for i, page in enumerate(pdf.pages):

        text = page.extract_text()
        if text:
            text_chunks.append(text)


        page_image = page.to_image(resolution=150).original

        for j, img in enumerate(page.images):

            x0, top, x1, bottom = img['x0'], img['top'], img['x1'], img['bottom']


            height = page.height
            box = (x0, height - bottom, x1, height - top)

            cropped = page_image.crop(box)
            img_path = f"images/page_{i}_img_{j}.png"
            cropped.save(img_path)
            image_paths.append(img_path)

print(f"âœ… Extracted {len(text_chunks)} text chunks and {len(image_paths)} images.")

from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import torch


clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")


def embed_texts(texts):
    inputs = clip_processor(text=texts, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        return clip_model.get_text_features(**inputs)

def embed_images(paths):
    images = [Image.open(p).convert("RGB") for p in paths]
    inputs = clip_processor(images=images, return_tensors="pt", padding=True)
    with torch.no_grad():
        return clip_model.get_image_features(**inputs)

import faiss
import numpy as np

# Safely compute embeddings
text_embeddings = embed_texts(text_chunks).cpu().numpy() if text_chunks else np.zeros((0, 512))
image_embeddings = embed_images(image_paths).cpu().numpy() if image_paths else np.zeros((0, 512))


combined_embeddings = np.concatenate([text_embeddings, image_embeddings], axis=0)
combined_metadata = text_chunks + image_paths


if combined_embeddings.shape[0] > 0:
    index = faiss.IndexFlatL2(512)
    index.add(combined_embeddings)
    print(f"FAISS index created with {len(combined_metadata)} items.")
else:
    index = None
    print("No embeddings to index.")

def search(query, top_k=3):
    if index is None:
        return ["âš ï¸ No indexed data"]

    query_embedding = embed_texts([query])[0].unsqueeze(0).cpu().numpy()
    D, I = index.search(query_embedding, top_k)
    return [combined_metadata[i] for i in I[0]]


results = search("spectrogram diagram")
print("ğŸ” Search Results:", results)

from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration

blip_model = InstructBlipForConditionalGeneration.from_pretrained("Salesforce/instructblip-vicuna-7b", device_map="auto")
blip_processor = InstructBlipProcessor.from_pretrained("Salesforce/instructblip-vicuna-7b")

def answer_with_image(image_path, question):
    image = Image.open(image_path).convert("RGB")
    inputs = blip_processor(images=image, text=question, return_tensors="pt").to("cuda")
    outputs = blip_model.generate(**inputs)
    return blip_processor.batch_decode(outputs, skip_special_tokens=True)[0]


for item in results:
    if item.endswith(".png"):
        print("ğŸ¤– Answer:", answer_with_image(item, "What does this image show?"))

def pretty_print_text(text, file):
    # Replace escaped newlines with real newlines if any, then split lines and write to file
    text = text.replace('\\n', '\n').replace('\n\n', '\n')
    lines = text.split('\n')
    for line in lines:
        file.write(line.strip() + '\n')
    file.write('\n')  # extra blank line after each chunk

results = search("spectrogram diagram")

with open("answer.txt", "w", encoding="utf-8") as f:
    f.write("ğŸ” Search Results:\n\n")
    for item in results:
        if item.endswith(".png"):
            answer = answer_with_image(item, "What does this image show?")
            f.write(f"ğŸ–¼ï¸ Image: {item}\nğŸ¤– Answer:\n{answer}\n{'-'*40}\n\n")
        else:
            f.write("ğŸ“„ Text Result:\n")
            pretty_print_text(item, f)
            f.write('-' * 40 + '\n\n')